import json
import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests
from bs4 import BeautifulSoup
from loguru import logger
from requests.adapters import HTTPAdapter, Retry

# --- å…¨åŸŸåƒæ•¸è¨­å®š ---
DATA_FOLDER = os.getenv("DATA_FOLDER", "temp")
OUTPUT_FOLDER = Path(DATA_FOLDER + "/buses")

HEADERS: Dict[str, str] = {
    "accept": "*/*",
    "accept-encoding": "gzip, deflate, br",
    "accept-language": "zh-TW,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,zh-CN;q=0.5",
    "dnt": "1",
    "host": "affairs.site.nthu.edu.tw",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
}

BUS_URL: Dict[str, Dict[str, Any]] = {
    "æ ¡æœ¬éƒ¨å…¬è»Š": {
        "url": "https://affairs.site.nthu.edu.tw/p/412-1165-20978.php?Lang=zh-tw",
        "info": ["towardTSMCBuildingInfo", "towardMainGateInfo"],
        "schedule": [
            "weekdayBusScheduleTowardTSMCBuilding",  # å¾€å°ç©é¤¨å¹³æ—¥æ™‚åˆ»è¡¨
            "weekendBusScheduleTowardTSMCBuilding",  # å¾€å°ç©é¤¨å‡æ—¥æ™‚åˆ»è¡¨
            "weekdayBusScheduleTowardMainGate",  # å¾€æ ¡é–€å£å¹³æ—¥æ™‚åˆ»è¡¨
            "weekendBusScheduleTowardMainGate",  # å¾€æ ¡é–€å£å‡æ—¥æ™‚åˆ»è¡¨
        ],
    },
    "å—å¤§å€é–“è»Š": {
        "url": "https://affairs.site.nthu.edu.tw/p/412-1165-20979.php?Lang=zh-tw",
        "info": ["towardMainCampusInfo", "towardSouthCampusInfo"],
        "schedule": [
            "weekdayBusScheduleTowardMainCampus",  # å¾€æ ¡æœ¬éƒ¨å¹³æ—¥æ™‚åˆ»è¡¨
            "weekendBusScheduleTowardMainCampus",  # å¾€æ ¡æœ¬éƒ¨å‡æ—¥æ™‚åˆ»è¡¨
            "weekdayBusScheduleTowardSouthCampus",  # å¾€å—å¤§å¹³æ—¥æ™‚åˆ»è¡¨
            "weekendBusScheduleTowardSouthCampus",  # å¾€å—å¤§å‡æ—¥æ™‚åˆ»è¡¨
        ],
    },
}

BUS_ANNOUCEMENT_URL = "https://affairs.site.nthu.edu.tw/p/403-1165-127-1.php"
BUS_ANNOUCEMENT_ROUTE_IMAGE_KEYWORD = {
    "main": ("æ ¡åœ’å…¬è»Š"),
    "nanda": ("å—å¤§", "å€é–“è»Š"),
}
BUS_ANNOUCEMENT_SCHEDULE_IMAGE_KEYWORD = "æ™‚åˆ»è¡¨"
API_URL = "https://api.nthusa.tw/scrapers/rpage/announcements/"

# è¨­å®š requests session èˆ‡ retry æ©Ÿåˆ¶
session = requests.Session()
retries = Retry(
    total=5,  # é‡è©¦ 5 æ¬¡
    backoff_factor=1,  # æŒ‡æ•¸é€€é¿ï¼š1ç§’ã€2ç§’ã€4ç§’...
    status_forcelist=[500, 502, 503, 504, 408],
)
session.mount("https://", HTTPAdapter(max_retries=retries))


def parse_campus_info(variable_string: str, res_text: str) -> Optional[Dict[str, Any]]:
    """
    å¾ç¶²é åŸå§‹ç¢¼ä¸­è§£ææŒ‡å®šè®Šæ•¸ï¼ˆç‰©ä»¶å‹è³‡æ–™ï¼‰ï¼Œä¸¦è™•ç†éƒ¨åˆ†æ¬„ä½ä¸­ HTML æ¨™ç±¤ã€‚

    åƒæ•¸:
        variable_string (str): è¦è§£æçš„è®Šæ•¸åç¨±
        res_text (str): ç¶²é åŸå§‹ç¢¼å…§å®¹

    å›å‚³:
        dict æˆ– None: è§£æå¾Œçš„è³‡æ–™ï¼Œè‹¥è§£æå¤±æ•—å‰‡å›å‚³ None
    """
    regex_pattern = r"const " + re.escape(variable_string) + r" = (\{.*?\})"
    data_match = re.search(regex_pattern, res_text, re.S)
    if data_match is None:
        logger.error(f"â æ‰¾ä¸åˆ°è®Šæ•¸ {variable_string} çš„è³‡æ–™")
        return None

    data = data_match.group(1)
    # ä½¿ç”¨æ›¿æ›æŠ€å·§è™•ç†å¼•è™Ÿå•é¡Œï¼šå…ˆå°‡å–®å¼•è™Ÿæš«å­˜ç‚ºå…¶ä»–ç¬¦è™Ÿï¼Œå†çµ±ä¸€æ›æˆé›™å¼•è™Ÿ
    data = data.replace("'", "|")
    data = data.replace('"', '\\"')
    data = data.replace("|", '"')
    data = data.replace("\n", "")
    data = data.replace("direction", '"direction"')
    data = data.replace("duration", '"duration"')
    data = data.replace("route", '"route"', 1)  # åƒ…æ›¿æ›ç¬¬ä¸€å€‹å‡ºç¾çš„ route
    data = data.replace("routeEN", '"routeEN"')

    try:
        data_dict = json.loads(data)
    except json.JSONDecodeError as e:
        logger.error(f"â è§£æ {variable_string} JSON å¤±æ•—: {e}")
        return None

    # ä½¿ç”¨ BeautifulSoup ç§»é™¤ route èˆ‡ routeEN ä¸­çš„ <span> æ¨™ç±¤
    for key in ["route", "routeEN"]:
        if key in data_dict and (
            "<span" in data_dict[key] or "</span>" in data_dict[key]
        ):
            soup = BeautifulSoup(data_dict[key], "html.parser")
            data_dict[key] = soup.get_text(separator=" ")
    return data_dict


def parse_bus_schedule(
    variable_string: str, res_text: str
) -> Optional[List[Dict[str, Any]]]:
    """
    å¾ç¶²é åŸå§‹ç¢¼ä¸­è§£ææŒ‡å®šè®Šæ•¸ï¼ˆé™£åˆ—å‹è³‡æ–™ï¼‰ï¼Œä¸¦è™•ç†éƒ¨åˆ†æ¬„ä½è³‡æ–™ã€‚

    åƒæ•¸:
        variable_string (str): è¦è§£æçš„è®Šæ•¸åç¨±
        res_text (str): ç¶²é åŸå§‹ç¢¼å…§å®¹

    å›å‚³:
        list æˆ– None: è§£æå¾Œçš„é™£åˆ—è³‡æ–™ï¼Œè‹¥è§£æå¤±æ•—å‰‡å›å‚³ None
    """
    regex_pattern = r"const " + re.escape(variable_string) + r" = (\[.*?\])"
    data_match = re.search(regex_pattern, res_text, re.S)
    if data_match is None:
        logger.error(f"â æ‰¾ä¸åˆ°è®Šæ•¸ {variable_string} çš„è³‡æ–™")
        return None

    data = data_match.group(1)
    data = data.replace("'", '"')
    data = data.replace("\n", "")
    data = data.replace("time", '"time"')
    data = data.replace("description", '"description"')
    data = data.replace("depStop", '"dep_stop"')
    data = data.replace("line", '"line"')
    data = re.sub(r",[ ]+?\]", "]", data)  # ç§»é™¤å¤šé¤˜çš„é€—è™Ÿ

    try:
        data_list = json.loads(data)
    except json.JSONDecodeError as e:
        logger.error(f"è§£æ {variable_string} JSON å¤±æ•—: {e}")
        return None

    # ç§»é™¤ time æ¬„ä½ç‚ºç©ºçš„é …ç›®
    data_list = [item for item in data_list if item.get("time", "") != ""]

    # æ ¹æ“šç¬¬ä¸€ç­†è³‡æ–™åˆ¤æ–·è·¯ç·šå±¬æ€§
    route_str = "æ ¡åœ’å…¬è»Š" if data_list and "line" in data_list[0] else "å—å¤§å€é–“è»Š"
    for item in data_list:
        item["route"] = route_str
    return data_list


def save_json_data(file_path: Path, data: Any, desc: str) -> None:
    try:
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.success(f'âœ… å„²å­˜ {desc} çš„è³‡æ–™åˆ° "{file_path}"')
    except IOError as e:
        logger.error(f"â å„²å­˜æª”æ¡ˆå¤±æ•— {file_path}: {e}")


def scrape_buses(path: Path) -> Dict[str, Dict]:
    """
    çˆ¬å–æ ¡æœ¬éƒ¨å…¬è»Šèˆ‡å—å¤§å€é–“è»Šçš„ç›¸é—œè³‡æ–™ï¼Œä¸¦å°‡è³‡æ–™å„²å­˜ç‚º JSON æª”æ¡ˆã€‚
    """
    path.mkdir(parents=True, exist_ok=True)
    all_data = {}
    for name, data in BUS_URL.items():
        logger.info(f"ğŸ”— æ­£åœ¨è™•ç†ï¼š{data['url']}")
        try:
            response = session.get(data["url"], headers=HEADERS, timeout=10)
            response.raise_for_status()
            res_text = response.text
            logger.success(f"âœ… æˆåŠŸå–å¾— {name} çš„è³‡æ–™")
        except requests.RequestException as e:
            logger.error(f"â å–å¾— {name} è³‡æ–™å¤±æ•—: {e}")
            continue

        # è™•ç† info èˆ‡ schedule è³‡æ–™
        for key, parser in [
            ("info", parse_campus_info),
            ("schedule", parse_bus_schedule),
        ]:
            for item in data[key]:
                parsed_data = parser(item, res_text)
                if parsed_data is not None:
                    logger.debug(parsed_data)
                    file_path = path / f"{item}.json"
                    all_data[item] = parsed_data
                    save_json_data(file_path, parsed_data, item)
                else:
                    logger.error(f"è§£æ {item} è³‡æ–™å¤±æ•—")
    return all_data


def combine_bus_data(data, path: Path) -> None:
    """
    çµåˆå…¬è»Šèˆ‡å—å¤§å€é–“è»Šçš„è³‡æ–™ï¼Œä¸¦å„²å­˜ç‚º JSON æª”æ¡ˆã€‚

    åƒæ•¸:
        data (Dict[str, Any]): å…¬è»Šèˆ‡å—å¤§å€é–“è»Šçš„è³‡æ–™
        path (Path): è³‡æ–™å„²å­˜çš„è³‡æ–™å¤¾è·¯å¾‘
    """
    with (path / "buses.json").open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    logger.success(f'âœ… æˆåŠŸå°‡å…¬è»Šèˆ‡å—å¤§å€é–“è»Šçš„è³‡æ–™å„²å­˜åˆ° "{path / "buses.json"}"')


def title_matches(title: str, groups: tuple) -> bool:
    """æª¢æŸ¥ title æ˜¯å¦åŒæ™‚åŒ…å« groups è£¡æ¯çµ„é—œéµå­—çš„ä»»ä¸€é—œéµå­—"""
    return all(
        any(keyword in title for keyword in BUS_ANNOUCEMENT_IMAGE_KEYWORD[group])
        for group in groups
    )


def extract_image_links(announcement_url: str) -> list:
    """
    å¾å…¬å‘Šé é¢ä¸­æå–åœ–ç‰‡é€£çµã€‚

    åƒæ•¸:
        announcement_url (str): å…¬å‘Šé é¢çš„ URL

    è¿”å›:
        list: åœ–ç‰‡é€£çµåˆ—è¡¨ï¼Œå¦‚æœæå–å¤±æ•—å‰‡è¿”å›ç©ºåˆ—è¡¨
    """
    image_links = []
    try:
        response = session.get(announcement_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        main_content = soup.find("div", class_="main")
        if main_content:
            meditor_content = main_content.find("div", class_="meditor")
            if meditor_content:
                images = meditor_content.find_all("img")
                for image in images:
                    image_url = image.get("src", "")
                    if image_url:
                        # ç¢ºä¿åœ–ç‰‡é€£çµæ˜¯çµ•å°è·¯å¾‘
                        image_url = (
                            "https://affairs.site.nthu.edu.tw" + image_url
                            if not image_url.startswith("http")
                            else image_url
                        )
                        image_links.append(image_url)
            else:
                logger.warning(f"â åœ¨ {announcement_url} ä¸­æ‰¾ä¸åˆ° .meditor å…ƒç´ ")
        else:
            logger.warning(f"â åœ¨ {announcement_url} ä¸­æ‰¾ä¸åˆ° .main å…ƒç´ ")
    except requests.RequestException as e:
        logger.error(f"â æå–åœ–ç‰‡é€£çµå¤±æ•—: {e}")
    except AttributeError as e:
        logger.error(f"â è§£æ HTML å¤±æ•—: {e}ï¼Œç¶²é çµæ§‹å¯èƒ½å·²è®Šæ›´")
    return image_links


def download_image(url: str, img_path: Path) -> bool:
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
        # ç¢ºä¿ç›®çš„è³‡æ–™å¤¾å­˜åœ¨
        img_path.parent.mkdir(parents=True, exist_ok=True)
        img_path.write_bytes(response.content)
        logger.success(f"âœ… æˆåŠŸä¸‹è¼‰åœ–ç‰‡è‡³: {img_path}")
        return True
    except requests.RequestException as e:
        logger.error(f"â ä¸‹è¼‰åœ–ç‰‡å¤±æ•—: {e}")
        return False


def scrape_bus_images(url: str, path: Path) -> None:
    # ç¢ºä¿åœ–ç‰‡å­˜æ”¾è·¯å¾‘å­˜åœ¨
    path.mkdir(parents=True, exist_ok=True)

    # å–å¾—å…¬å‘Šé é¢
    response = session.get(f"{API_URL}{url}", timeout=10)
    announcements = response.json()

    announcement_links = {}
    for item in announcements:
        title = item.get("title", "")
        # å…ˆåˆ¤æ–·æ˜¯å¦åŒ…å«æ™‚åˆ»è¡¨é—œéµå­—
        if not any(kw in title for kw in BUS_ANNOUCEMENT_SCHEDULE_IMAGE_KEYWORD):
            continue
        # åˆ¤æ–·è·¯ç”±é—œéµå­—
        for key, keywords in BUS_ANNOUCEMENT_ROUTE_IMAGE_KEYWORD.items():
            if key not in announcement_links and any(kw in title for kw in keywords):
                announcement_links[key] = item.get("link", "")

    for key, announcement_url in announcement_links.items():
        image_urls = extract_image_links(announcement_url)
        if not image_urls:
            logger.warning(f"â åœ¨ {key} å…¬å‘Šä¸­æ‰¾ä¸åˆ°åœ–ç‰‡ã€‚")
            continue
        for idx, image_url in enumerate(image_urls):
            image_path = path / f"{key}_{idx}.jpg"
            logger.info(f"ğŸ”— æ­£åœ¨ä¸‹è¼‰åœ–ç‰‡ï¼š{image_url}")
            download_image(image_url, image_path)


if __name__ == "__main__":
    all_data = scrape_buses(OUTPUT_FOLDER)
    combine_bus_data(all_data, Path(DATA_FOLDER))
    scrape_bus_images(
        BUS_ANNOUCEMENT_URL,
        OUTPUT_FOLDER / "images",
    )
