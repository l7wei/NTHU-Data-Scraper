import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Set

import scrapy
from scrapy.http import Response

# --- å…¨åŸŸåƒæ•¸è¨­å®š ---
DATA_FOLDER = Path(os.getenv("DATA_FOLDER", "temp"))
OUTPUT_FOLDER = DATA_FOLDER / "newsletters"
COMBINED_JSON_FILE = DATA_FOLDER / "newsletters.json"
URL_PREFIX = "https://newsletter.cc.nthu.edu.tw"


# --- è³‡æ–™çµæ§‹å®šç¾© ---
class NewsletterItem(scrapy.Item):
    """
    é›»å­å ±è³‡æ–™é …ç›®ã€‚åŒ…å«åç¨±ã€é€£çµã€è¡¨æ ¼è³‡æ–™ä»¥åŠå¯¦éš›çš„æ–‡ç« å…§å®¹ã€‚
    """

    name = scrapy.Field()
    link = scrapy.Field()
    table = scrapy.Field()
    articles = scrapy.Field()


class NewsletterArticle(scrapy.Item):
    """
    é›»å­å ±ä¸­å–®ç¯‡æ–‡ç« çš„è³‡æ–™çµæ§‹ã€‚
    """

    title = scrapy.Field()
    link = scrapy.Field()
    date = scrapy.Field()


class NewsletterSpider(scrapy.Spider):
    """
    æ¸…è¯å¤§å­¸é›»å­å ±çˆ¬èŸ²ã€‚

    æ­¤çˆ¬èŸ²æœƒæŠ“å–æ¸…è¯å¤§å­¸æ‰€æœ‰é›»å­å ±çš„åˆ—è¡¨ï¼Œä»¥åŠå„å€‹é›»å­å ±ä¸­çš„æ–‡ç« åˆ—è¡¨ã€‚
    çˆ¬å–éç¨‹ä½¿ç”¨ Scrapy æ¡†æ¶ï¼Œå¯ç¢ºä¿é«˜æ•ˆç‡çš„è³‡æ–™æ”¶é›†ã€‚
    """

    name = "nthu_newsletters"
    allowed_domains = ["newsletter.cc.nthu.edu.tw"]
    start_urls = [f"{URL_PREFIX}/nthu-list/search.html"]
    custom_settings = {
        "LOG_LEVEL": "INFO",
        "ITEM_PIPELINES": {"nthu_scraper.spiders.nthu_newsletters.JsonPipeline": 1},
    }

    processed_urls: Set[str] = set()  # ç”¨æ–¼è¿½è¹¤å·²è™•ç†çš„ URLï¼Œé¿å…é‡è¤‡è«‹æ±‚

    def parse(self, response: Response) -> scrapy.Request:
        """
        è§£æé›»å­å ±åˆ—è¡¨é é¢ï¼Œæå–å„é›»å­å ±çš„åç¨±ã€é€£çµèˆ‡è¡¨æ ¼è³‡æ–™ã€‚

        Args:
            response (Response): Scrapy ä¸‹è¼‰å™¨è¿”å›çš„å›æ‡‰ç‰©ä»¶

        Yields:
            Request: ç‚ºæ¯å€‹é›»å­å ±ç™¼é€è«‹æ±‚ï¼Œå–å¾—å…¶æ–‡ç« åˆ—è¡¨
        """
        self.logger.info(f"ğŸ”— æ­£åœ¨è™•ç†é›»å­å ±åˆ—è¡¨é é¢ï¼š{response.url}")

        gallery = response.css("div.gallery")
        if not gallery:
            self.logger.error("â æ‰¾ä¸åˆ°é›»å­å ±åˆ—è¡¨")
            return

        for li in gallery.css("li"):
            h3 = li.css("h3")
            if not h3:
                continue

            a = h3.css("a")
            if not a:
                continue

            name = a.css("::text").get().strip()
            link = a.css("::attr(href)").get()

            if not link or not name:
                continue

            # æå–è¡¨æ ¼è³‡æ–™
            table_data = {}
            table = li.css("table")
            if table:
                for row in table.css("tr"):
                    elements = row.css("td, th")  # åŒæ™‚é¸æ“‡ td èˆ‡ th
                    if len(elements) == 2:
                        key = elements[0].css("::text").get().strip()
                        value = elements[1].css("::text").get().strip()
                        if key and value:  # ç¢ºä¿å…©è€…éƒ½éç©º
                            table_data[key] = value

            newsletter = NewsletterItem()
            newsletter["name"] = name
            newsletter["link"] = link
            newsletter["table"] = table_data
            newsletter["articles"] = []

            # å¦‚æœé€£çµå·²ç¶“åœ¨è™•ç†æ¸…å–®ä¸­ï¼Œè·³é
            if link in self.processed_urls:
                continue

            self.processed_urls.add(link)

            # ç™¼é€è«‹æ±‚ç²å–æ­¤é›»å­å ±çš„æ–‡ç« åˆ—è¡¨
            yield scrapy.Request(
                url=link,
                callback=self.parse_newsletter_content,
                meta={"newsletter": newsletter},
                dont_filter=False,  # ä¸é‡è¤‡è™•ç†ç›¸åŒçš„ URL
            )

    def parse_newsletter_content(self, response: Response) -> NewsletterItem:
        """
        è§£æå–®å€‹é›»å­å ±é é¢ï¼Œæå–æ–‡ç« æ¨™é¡Œã€é€£çµå’Œæ—¥æœŸã€‚

        Args:
            response (Response): é›»å­å ±é é¢çš„å›æ‡‰ç‰©ä»¶

        Yields:
            NewsletterItem: åŒ…å«é›»å­å ±è©³ç´°è³‡è¨ŠåŠå…¶æ–‡ç« åˆ—è¡¨çš„ Item ç‰©ä»¶
        """
        newsletter = response.meta["newsletter"]
        self.logger.info(f"ğŸ”— æ­£åœ¨è™•ç†é›»å­å ±ï¼š{newsletter['name']} {response.url}")

        content = response.css("div#acyarchivelisting")
        if not content:
            self.logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°é›»å­å ±å…§å®¹ï¼š{newsletter['name']}")
            yield newsletter
            return

        table = content.css("table.contentpane")
        if not table:
            self.logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°æ–‡ç« è¡¨æ ¼ï¼š{newsletter['name']}")
            yield newsletter
            return

        articles = []
        for archive_row in table.css("div.archiveRow"):
            article = NewsletterArticle()

            # æå–æ–‡ç« æ¨™é¡Œèˆ‡é€£çµ
            a = archive_row.css("a")
            if a:
                onclick = a.css("::attr(onclick)").get()
                if onclick:
                    match = re.search(r"openpopup\('(.*?)',", onclick)
                    if match:
                        article["link"] = f"{URL_PREFIX}{match.group(1)}"
                article["title"] = a.css("::text").get().strip()

            # æå–æ–‡ç« æ—¥æœŸ
            date_span = archive_row.css("span.sentondate")
            if date_span:
                date_str = date_span.css("::text").get().strip()
                if date_str:
                    date_str = date_str.replace("Sent on ", "")
                    date_str = self._convert_chinese_month_to_english(date_str)
                    try:
                        parsed_date = datetime.strptime(date_str, "%d %b %Y")
                        article["date"] = parsed_date.strftime("%Y-%m-%d")
                    except ValueError:
                        self.logger.error(f"â æ—¥æœŸè§£æéŒ¯èª¤: {date_str}")
                        article["date"] = date_str  # ä¿ç•™åŸå§‹æ—¥æœŸå­—ä¸²

            # åªæœ‰ç•¶æ–‡ç« è‡³å°‘æœ‰æ¨™é¡Œæ™‚æ‰åŠ å…¥åˆ—è¡¨
            if article.get("title"):
                articles.append(dict(article))

        newsletter["articles"] = articles
        yield newsletter

    def _convert_chinese_month_to_english(self, date_str: str) -> str:
        """
        å°‡ä¸­æ–‡æœˆä»½è½‰æ›ç‚ºè‹±æ–‡ç¸®å¯«ã€‚

        Args:
            date_str (str): åŒ…å«ä¸­æ–‡æœˆä»½çš„æ—¥æœŸå­—ä¸²

        Returns:
            str: è½‰æ›å¾Œçš„æ—¥æœŸå­—ä¸²ï¼Œæœˆä»½ç‚ºè‹±æ–‡ç¸®å¯«
        """
        month_mapping = {
            " ä¸€æœˆ ": " Jan ",
            " äºŒæœˆ ": " Feb ",
            " ä¸‰æœˆ ": " Mar ",
            " å››æœˆ ": " Apr ",
            " äº”æœˆ ": " May ",
            " å…­æœˆ ": " Jun ",
            " ä¸ƒæœˆ ": " Jul ",
            " å…«æœˆ ": " Aug ",
            " ä¹æœˆ ": " Sep ",
            " åæœˆ ": " Oct ",
            " åä¸€æœˆ ": " Nov ",
            " åäºŒæœˆ ": " Dec ",
        }
        for zh_month, en_month in month_mapping.items():
            date_str = date_str.replace(zh_month, en_month)
        return date_str


class JsonPipeline:
    """
    Scrapy Pipelineï¼Œç”¨æ–¼å°‡çˆ¬å–çš„ Item å„²å­˜ç‚º JSON æª”æ¡ˆã€‚
    åŒæ™‚æœƒåˆä½µæ‰€æœ‰é›»å­å ±è³‡æ–™åˆ°ä¸€å€‹ç¸½åˆæª”æ¡ˆã€‚
    """

    def open_spider(self, spider):
        """
        Spider é–‹å•Ÿæ™‚åŸ·è¡Œï¼Œå»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾ã€‚
        """
        OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)
        COMBINED_JSON_FILE.parent.mkdir(parents=True, exist_ok=True)
        self.combined_data = []
        # ç”¨ä¾†è¨ˆæ•¸é‡è¤‡åç¨±
        self.name_counts = {}

    def process_item(self, item, spider):
        """
        è™•ç†æ¯ä¸€å€‹ Itemï¼Œå„²å­˜é›»å­å ±è³‡æ–™åˆ° JSON æª”æ¡ˆã€‚

        Args:
            item (NewsletterItem): çˆ¬å–åˆ°çš„é›»å­å ±è³‡æ–™
            spider (NewsletterSpider): çˆ¬èŸ²å¯¦ä¾‹

        Returns:
            NewsletterItem: è™•ç†å¾Œçš„ Item
        """
        # ç”Ÿæˆå®‰å…¨çš„æª”å
        base_name = item["name"].replace("/", "-").replace("\\", "-")

        if base_name not in self.name_counts:
            self.name_counts[base_name] = 0
            safe_name = base_name
        else:
            self.name_counts[base_name] += 1
            safe_name = f"{base_name}_{self.name_counts[base_name]}"

        serializable_item = dict(item)
        filename = OUTPUT_FOLDER / f"{safe_name}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(serializable_item, f, ensure_ascii=False, indent=4)

        spider.logger.info(f'âœ… æˆåŠŸå„²å­˜ã€{item['name']}ã€‘è³‡æ–™è‡³ "{safe_name}.json"')
        self.combined_data.append(serializable_item)

        return item

    def close_spider(self, spider):
        """
        Spider é—œé–‰æ™‚åŸ·è¡Œï¼Œåˆä½µæ‰€æœ‰é›»å­å ± JSON æª”æ¡ˆã€‚
        """
        sorted_data = sorted(self.combined_data, key=lambda x: x["name"])
        with open(COMBINED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted_data, f, ensure_ascii=False, indent=4)
        spider.logger.info(f'âœ… æˆåŠŸå„²å­˜é›»å­å ±è³‡æ–™è‡³ "{COMBINED_JSON_FILE}"')
