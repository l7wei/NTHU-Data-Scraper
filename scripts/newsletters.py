import json
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import requests
from bs4 import BeautifulSoup
from loguru import logger
from requests.adapters import HTTPAdapter, Retry

# --- å…¨åŸŸåƒæ•¸è¨­å®š ---
DATA_FOLDER = os.getenv("DATA_FOLDER", "temp")
OUTPUT_PATH = Path(DATA_FOLDER + "/newsletters")
LIST_OUTPUT_PATH = Path(DATA_FOLDER + "/newsletters_list.json")

URL_PREFIX = "https://newsletter.cc.nthu.edu.tw"
HEADERS = {
    "accept": "*/*",
    "accept-encoding": "gzip, deflate, br",
    "accept-language": "zh-TW,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,zh-CN;q=0.5",
    "dnt": "1",
    "host": "newsletter.cc.nthu.edu.tw",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
}

# è¨­å®š requests sessionï¼Œå¢åŠ  retry æ©Ÿåˆ¶
session = requests.Session()
retries = Retry(
    total=5,  # ç¸½å…±é‡è©¦ 5 æ¬¡
    backoff_factor=1,  # æŒ‡æ•¸é€€é¿ï¼ˆ1 ç§’ã€2 ç§’ã€4 ç§’â€¦ï¼‰
    status_forcelist=[500, 502, 503, 504, 408],
)
session.mount("https://", HTTPAdapter(max_retries=retries))


def parse_newsletter_list_html(res_text: str) -> List[Dict[str, Any]]:
    """
    è§£æé›»å­å ±åˆ—è¡¨é é¢ï¼Œå–å¾—å„é›»å­å ±çš„åç¨±ã€é€£çµèˆ‡è¡¨æ ¼è³‡æ–™
    """
    soup = BeautifulSoup(res_text, "html.parser")
    gallery = soup.find("div", {"class": "gallery"})
    newsletter_list = []

    if gallery is None:
        logger.error("âŒ æ‰¾ä¸åˆ°é›»å­å ±åˆ—è¡¨")
        return []

    for li in gallery.find_all("li"):
        h3 = li.find("h3")
        if h3:
            a = h3.find("a")
            if not a:
                continue
            name = a.text.strip()
            link = a["href"]
            table_data = {}
            table = h3.find("table")
            if table:
                for row in table.find_all("tr"):
                    elements = row.find_all(["td", "th"])  # è€ƒæ…®åˆ°å¯èƒ½æœ‰ th èˆ‡ td
                    if len(elements) == 2:
                        key = elements[0].text.strip()
                        value = elements[1].text.strip()
                        table_data[key] = value
            newsletter_list.append({"name": name, "link": link, "table": table_data})

    return newsletter_list


def scrape_newsletters_list(path: Path = LIST_OUTPUT_PATH) -> List[Dict[str, Any]]:
    """
    çˆ¬å–é›»å­å ±æ¸…å–®ä¸¦å­˜å…¥ JSON æª”æ¡ˆï¼Œè¿”å›é›»å­å ±åˆ—è¡¨
    """
    url = URL_PREFIX + "/nthu-list/search.html"
    logger.info(f"æ­£åœ¨è™•ç†ï¼š{url}")
    try:
        response = session.get(url, headers=HEADERS, timeout=10)
        response.encoding = "big5"
        newsletter_list = parse_newsletter_list_html(response.text)
        logger.debug(newsletter_list)

        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("w", encoding="utf-8") as f:
            json.dump(newsletter_list, f, ensure_ascii=False, indent=4)
        logger.success(f'âœ… æˆåŠŸå„²å­˜é›»å­å ±æ¸…å–®è‡³ "{path}"')

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ ç„¡æ³•çˆ¬å–é›»å­å ±æ¸…å–®: {e}")
        return []

    return newsletter_list


def parse_selected_newsletter(res_text: str) -> List[Dict[str, Any]]:
    """
    è§£æé¸å®šçš„é›»å­å ±é é¢ï¼Œæå–æ–‡ç« æ¨™é¡Œã€é€£çµå’Œæ—¥æœŸ
    """
    soup = BeautifulSoup(res_text, "html.parser")
    content = soup.find("div", {"id": "acyarchivelisting"})
    if content is None:
        return []

    table = content.find("table", {"class": "contentpane"})
    if table is None:
        return []

    newsletter_list = []
    for archive_row in table.find_all("div", {"class": "archiveRow"}):
        title, link, date_str = None, None, None

        a = archive_row.find("a")
        if a:
            onclick = a.get("onclick", "")
            match = re.search(r"openpopup\('(.*?)',", onclick)
            if match:
                link = URL_PREFIX + match.group(1)
            title = a.text.strip()

        date_span = archive_row.find("span", {"class": "sentondate"})
        if date_span:
            date_str = date_span.text.strip()
            month_mapping = {
                " ä¸€æœˆ ": " Jan ",
                " äºŒæœˆ ": " Feb ",
                " ä¸‰æœˆ ": " Mar ",
                " å››æœˆ ": " Apr ",
                " äº”æœˆ ": " May ",
                " å…­æœˆ ": " Jun ",
                " ä¸ƒæœˆ ": " Jul ",
                " å…«æœˆ ": " Aug ",
                " ä¹æœˆ ": " Sep ",
                " åæœˆ ": " Oct ",
                " åä¸€æœˆ ": " Nov ",
                " åäºŒæœˆ ": " Dec ",
            }
            date_str = date_str.replace("Sent on ", "")
            for zh_month, en_month in month_mapping.items():
                date_str = date_str.replace(zh_month, en_month)
            try:
                date_str = datetime.strptime(date_str, "%d %b %Y").strftime("%Y-%m-%d")
            except ValueError:
                logger.error(f"æ—¥æœŸè§£æéŒ¯èª¤: {date_str}")
                date_str = None

        newsletter_list.append({"title": title, "link": link, "date": date_str})

    return newsletter_list


def scrape_selected_newsletter(url: str, name: str) -> None:
    """
    çˆ¬å–æŒ‡å®šé›»å­å ±çš„å…§å®¹ä¸¦å„²å­˜ç‚º JSON æª”æ¡ˆ
    """
    try:
        logger.info(f"ğŸ”— æ­£åœ¨è™•ç†ï¼š{name}:{url}")
        response = session.get(url, headers=HEADERS, timeout=10)
        response.encoding = "utf-8"
        newsletter_articles = parse_selected_newsletter(response.text)
        logger.debug(newsletter_articles)

        if not newsletter_articles:
            logger.warning(f"â— æ²’æœ‰ç²å–åˆ°ä»»ä½•å…§å®¹: {name}")

        safe_name = name.replace("/", "-")
        file_path = OUTPUT_PATH / f"{safe_name}.json"
        OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(newsletter_articles, f, ensure_ascii=False, indent=4)
        logger.success(f'âœ… æˆåŠŸå„²å­˜ {name} çš„å…§å®¹è‡³ "{file_path}"')
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ ç„¡æ³•æŠ“å– {name} ({url})ï¼ŒéŒ¯èª¤ç‚º {e}")


def scrape_all_newsletters(file_folder: Path) -> None:
    """
    çˆ¬å–æ‰€æœ‰é›»å­å ±ï¼ˆä½¿ç”¨å¤šåŸ·è¡Œç·’ï¼‰
    """
    file_folder.mkdir(parents=True, exist_ok=True)
    newsletters = scrape_newsletters_list()

    if not newsletters:
        logger.error("âŒ ç„¡æ³•ç²å–é›»å­å ±æ¸…å–®ï¼ŒçµæŸçˆ¬å–")
        return

    # ä½¿ç”¨ ThreadPoolExecutor æ›¿ä»£æ‰‹å‹•å»ºç«‹ threading.Thread
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [
            executor.submit(
                scrape_selected_newsletter, newsletter["link"], newsletter["name"]
            )
            for newsletter in newsletters
        ]
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–é›»å­å ±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    scrape_all_newsletters(OUTPUT_PATH)
