import json
import os
import re
from pathlib import Path
from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

import scrapy

# --- å…¨åŸŸåƒæ•¸è¨­å®š ---
DATA_FOLDER = Path(os.getenv("DATA_FOLDER", "temp"))
COMBINED_JSON_FILE = DATA_FOLDER / "announcements.json"

DIRECTORIES_PATH = Path("data/directories.json")  # å–®ä½è³‡æ–™ JSON æª”æ¡ˆçš„è·¯å¾‘

LANGUAGES = ["zh-tw", "en"]  # æ”¯æ´çš„èªè¨€åˆ—è¡¨
LANGUAGE_QUERY_PARAM = "Lang"  # èªè¨€æŸ¥è©¢åƒæ•¸å
RPAGE_DOMAIN_SUFFIX = "site.nthu.edu.tw"  # Rpage ç¶²åŸŸå¾Œç¶´

other_data = {
    "æ¸…è¯å…¬ä½ˆæ¬„": {
        "zh-tw": "https://bulletin.site.nthu.edu.tw/?Lang=zh-tw",
        "en": "https://bulletin.site.nthu.edu.tw/?Lang=en",
    },
    "åœ‹ç«‹æ¸…è¯å¤§å­¸å­¸ç”Ÿæœƒ": {
        "zh-tw": "https://nthusa.site.nthu.edu.tw/?Lang=zh-tw",
    },
}


class AnnouncementItem(scrapy.Item):
    """
    å…¬å‘Šè³‡è¨Š Itemã€‚

    æ­¤ Item å„²å­˜æ¯å€‹å…¬å‘Šçš„åç¨±ã€é€£çµã€èªè¨€ã€æ‰€å±¬å–®ä½å’Œæ–‡ç« åˆ—è¡¨ã€‚

    Attributes:
        title (scrapy.Field): å…¬å‘Šé¡åˆ¥æ¨™é¡Œ (ä¾‹å¦‚ï¼šæœ€æ–°å…¬å‘Š, å­¸è¡“æ´»å‹•)ã€‚
        link (scrapy.Field): å…¬å‘Šåˆ—è¡¨é é¢é€£çµã€‚
        language (scrapy.Field): å…¬å‘Šèªè¨€ã€‚
        department (scrapy.Field): æ‰€å±¬å–®ä½åç¨±ã€‚
        articles (scrapy.Field): å…¬å‘Šæ–‡ç« åˆ—è¡¨ï¼ŒåŒ…å« AnnouncementArticle ç‰©ä»¶ã€‚
    """

    title = scrapy.Field()
    link = scrapy.Field()
    language = scrapy.Field()
    department = scrapy.Field()
    articles = scrapy.Field()


class AnnouncementArticle(scrapy.Item):
    """
    å…¬å‘Šæ–‡ç« è³‡è¨Š Itemã€‚

    æ­¤ Item å„²å­˜å¾å…¬å‘Šç¶²é ä¸Šçˆ¬å–åˆ°çš„å–®ç¯‡å…¬å‘Šæ–‡ç« è³‡è¨Šï¼ŒåŒ…å«æ¨™é¡Œã€é€£çµã€æ—¥æœŸç­‰ã€‚

    Attributes:
        title (scrapy.Field): å…¬å‘Šæ–‡ç« æ¨™é¡Œã€‚
        link (scrapy.Field): å…¬å‘Šæ–‡ç« é€£çµã€‚
        date (scrapy.Field): å…¬å‘Šç™¼å¸ƒæ—¥æœŸã€‚
    """

    title = scrapy.Field()
    link = scrapy.Field()
    date = scrapy.Field()


def _update_url_lang_param(url: str, language: str) -> str:
    """
    æ›´æ–°ç¶²å€çš„èªè¨€æŸ¥è©¢åƒæ•¸ã€‚

    Args:
        url: ç¶²å€å­—ä¸²ã€‚
        language: èªè¨€ä»£ç¢¼ã€‚

    Returns:
        æ›´æ–°èªè¨€åƒæ•¸å¾Œçš„ç¶²å€å­—ä¸²ã€‚
    """
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    query_params[LANGUAGE_QUERY_PARAM] = [language]
    new_query = urlencode(query_params, doseq=True)
    return urlunparse(parsed_url._replace(query=new_query))


def build_lang_urls(original_url: str) -> dict[str, str] | None:
    """
    ç‚ºçµ¦å®šçš„åŸå§‹ URL å»ºç«‹åŒ…å«ä¸åŒèªè¨€ç‰ˆæœ¬çš„ URL å­—å…¸ã€‚

    å¦‚æœåŸå§‹ URL ä¸å±¬æ–¼ Rpage ç¶²åŸŸï¼Œå‰‡å›å‚³ Noneã€‚

    Args:
        original_url: åŸå§‹ç¶²å€å­—ä¸²ã€‚

    Returns:
        ä¸€å€‹å­—å…¸ï¼Œéµç‚ºèªè¨€ä»£ç¢¼ (å¦‚ "zh-tw", "en")ï¼Œå€¼ç‚ºå°æ‡‰èªè¨€ç‰ˆæœ¬çš„ URLã€‚
        å¦‚æœ URL ä¸å±¬æ–¼ Rpage ç¶²åŸŸï¼Œå‰‡å›å‚³ Noneã€‚
    """
    parsed_url = urlparse(original_url)
    if not parsed_url.hostname or not parsed_url.hostname.endswith(RPAGE_DOMAIN_SUFFIX):
        return None

    lang_urls = {}
    for lang in LANGUAGES:
        lang_urls[lang] = _update_url_lang_param(original_url, lang)
    return lang_urls


def load_rpage_urls_from_directories(
    directories_path: Path,
) -> dict[str, dict[str, str]]:
    """
    å¾ JSON æª”æ¡ˆè®€å–å–®ä½è³‡æ–™ä¸¦å»ºç«‹ Rpage URL å­—å…¸ã€‚

    Args:
        directories_path: å–®ä½è³‡æ–™ JSON æª”æ¡ˆçš„è·¯å¾‘ã€‚

    Returns:
        ä¸€å€‹å­—å…¸ï¼Œéµç‚ºå–®ä½åç¨±ï¼Œå€¼ç‚ºåŒ…å«å„èªè¨€ç‰ˆæœ¬ URL çš„å­—å…¸ã€‚
        å¦‚æœè¼‰å…¥æˆ–è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œå‰‡å›å‚³ç©ºå­—å…¸ã€‚
    """
    rpage_urls = {}
    with open(directories_path, "r", encoding="UTF-8") as f:
        directories = json.load(f)

    for department in directories:
        try:
            dept_name = department["name"]
            department_url = department["details"]["contact"]["ç¶²é "]
        except KeyError:
            print(f"å–®ä½ã€{department.get('name', 'æœªçŸ¥')}ã€‘ç¼ºå°‘å¿…è¦è³‡è¨Šï¼Œè·³éã€‚")
            continue
        lang_urls = build_lang_urls(department_url)
        if dept_name and lang_urls:
            rpage_urls[dept_name] = lang_urls
    return rpage_urls


class AnnouncementsSpider(scrapy.Spider):
    """
    çˆ¬å– Rpage ç¶²é å…¬å‘Šçš„ Scrapy Spiderã€‚

    æ­¤ Spider æœƒé‡å°è¨­å®šçš„å–®ä½ç¶²å€ï¼Œçˆ¬å–ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡å…©ç¨®èªè¨€ç‰ˆæœ¬çš„å…¬å‘Šï¼Œ
    ä¸¦å°‡å…¬å‘Šæ¨™é¡Œå’Œç¶²å€å„²å­˜ä¸‹ä¾†ã€‚
    """

    name = "nthu_announcements"
    custom_settings = {
        "ITEM_PIPELINES": {"nthu_scraper.spiders.nthu_announcements.JsonPipeline": 1},
    }

    def __init__(self, *args, **kwargs):
        """
        åˆå§‹åŒ– Spiderï¼Œè¼‰å…¥å–®ä½ç¶²å€ã€‚
        """
        super().__init__(*args, **kwargs)
        self.rpage_urls = {}
        # å¾æª”æ¡ˆè¼‰å…¥
        self.rpage_urls = load_rpage_urls_from_directories(DIRECTORIES_PATH)
        # åŠ å…¥å…¶ä»–è³‡æ–™
        self.rpage_urls.update(other_data)

    def start_requests(self):
        """
        Spider çš„èµ·å§‹è«‹æ±‚æ–¹æ³•ã€‚

        ç‚ºæ¯å€‹å–®ä½å’Œèªè¨€ç‰ˆæœ¬å»ºç«‹ Scrapy Requestï¼Œä¸¦æŒ‡å®š parse æ–¹æ³•ä½œç‚ºå›å‘¼å‡½æ•¸ã€‚
        """
        for dept, lang_urls in self.rpage_urls.items():
            for lang, url in lang_urls.items():
                yield scrapy.Request(
                    url,
                    callback=self.parse,
                    meta={
                        "department_name": dept,
                        "language": lang,
                        "original_url": url,
                    },
                )

    def parse(self, response):
        """
        è§£æç¶²é ä¸»é çš„å›å‘¼å‡½æ•¸ã€‚

        å„²å­˜ç¶²é  HTML å…§å®¹ï¼Œä¸¦å°‹æ‰¾å…¬å‘Šåˆ—è¡¨å’Œ "more" é€£çµã€‚

        Args:
            response: Scrapy Response ç‰©ä»¶ã€‚
        """
        department = response.meta.get("department_name")
        language = response.meta.get("language")
        if response.css("div.tab-content"):
            self.logger.info(
                f"ğŸ”— åœ¨ {department} ({language}) æ‰¾åˆ° tab æ¨¡çµ„: {response.url}"
            )
            yield from self.parse_tab_content(response, response.meta)

        yield from self.parse_more_links(response, response.meta)

    def parse_more_links(self, response, meta):
        """
        è§£æç¶²é ä¸­çš„ "more" é€£çµï¼Œä¸¦ç”¢ç”Ÿå°æ‡‰çš„ Scrapy Requestã€‚

        Args:
            response: Scrapy Response ç‰©ä»¶ã€‚
            meta: åŒ…å«å–®ä½åç¨±å’Œèªè¨€çš„å­—å…¸ã€‚
        """
        department = response.meta.get("department_name")
        language = response.meta.get("language")
        more_links = response.css("p.more a")
        for a in more_links:
            relative_url = a.css("::attr(href)").get()
            if relative_url:
                self.logger.info(
                    f"ğŸ”— åœ¨ {department} ({language}) æ‰¾åˆ°æ–°çš„å…¬å‘Š: {relative_url}"
                )
                absolute_url = response.urljoin(relative_url)
                # æ›´æ–°å‹•æ…‹è¼‰å…¥çš„é€£çµä»¥åŒ…å«èªè¨€æŸ¥è©¢åƒæ•¸
                updated_url = _update_url_lang_param(absolute_url, meta.get("language"))
                if RPAGE_DOMAIN_SUFFIX in urlparse(updated_url).hostname:
                    new_meta = meta.copy()
                    new_meta["announcement_url"] = updated_url
                    yield scrapy.Request(
                        updated_url,
                        callback=self.parse_announcement_list,
                        meta=new_meta,
                    )

    def parse_tab_content(self, response, meta):
        """
        è§£æç¶²é ä¸­çš„ tab contentï¼Œæå–å‹•æ…‹è¼‰å…¥çš„å…¬å‘Šé€£çµã€‚

        Args:
            response: Scrapy Response ç‰©ä»¶ã€‚
            meta: åŒ…å«å–®ä½åç¨±å’Œèªè¨€çš„å­—å…¸ã€‚
        """
        tabpane = response.css("div.tab-pane")
        for tab in tabpane:
            tab_text = tab.xpath("string(.)").get()
            url_match = re.search(
                r"\$\.\s*hajaxOpenUrl\(\s*'([^']+)'", tab_text, re.DOTALL
            )
            if url_match:
                extracted_url = url_match.group(1)
                absolute_extracted_url = response.urljoin(extracted_url)
                # æ›´æ–°å‹•æ…‹è¼‰å…¥çš„é€£çµä»¥åŒ…å«èªè¨€æŸ¥è©¢åƒæ•¸
                updated_extracted_url = _update_url_lang_param(
                    absolute_extracted_url, meta.get("language")
                )
                new_meta = meta.copy()
                # å†å‘¼å«ä¸€æ¬¡ parseï¼Œè™•ç†æ›´å¤šæŒ‰éˆ•
                yield scrapy.Request(
                    updated_extracted_url,
                    callback=self.parse,
                    meta=new_meta,
                )

    def parse_announcement_list(self, response):
        """
        è§£æå…¬å‘Šåˆ—è¡¨é é¢ï¼Œæå–å…¬å‘Šè³‡è¨Šä¸¦å„²å­˜ã€‚

        Args:
            response: Scrapy Response ç‰©ä»¶ã€‚
        """
        announcement_item = AnnouncementItem()
        announcement_item["title"] = self.process_title(response)
        announcement_item["link"] = response.url
        announcement_item["language"] = response.meta.get("language")
        announcement_item["department"] = response.meta.get("department_name")
        announcement_item["articles"] = self.process_content(response)
        yield announcement_item

    def process_title(self, response):
        """
        å¾å…¬å‘Šå…§é æå–å…¬å‘Šæ¨™é¡Œã€‚

        å„ªå…ˆä½¿ç”¨åŒ…å« "title" å­—æ¨£çš„ classï¼Œè‹¥æ‰¾ä¸åˆ°å‰‡ä½¿ç”¨ç¶²é  titleã€‚

        Args:
            response: Scrapy Response ç‰©ä»¶ã€‚

        Returns:
            å…¬å‘Šæ¨™é¡Œå­—ä¸²ï¼Œå¦‚æœæ‰¾ä¸åˆ°æ¨™é¡Œå‰‡å›å‚³ Noneã€‚
        """
        title_div = response.css("[class*='title']::text").get()
        if title_div:
            title_div = title_div.strip()
            if title_div != "":
                return title_div

        head_title = response.css("title::text").get()
        if head_title:
            return head_title.strip()

        self.logger.warning(f"è­¦å‘Šï¼šåœ¨ {response.url} æ‰¾ä¸åˆ°å…¬å‘Šæ¨™é¡Œã€‚")
        return None

    def process_content(self, response) -> list[AnnouncementArticle]:
        """
        è§£æå…¬å‘Šåˆ—è¡¨é é¢ã€‚

        æ­¤æ–¹æ³•è§£æä¸‹è¼‰çš„å…¬å‘Šåˆ—è¡¨é é¢ï¼Œä½¿ç”¨ CSS selector é¸å–å…¬å‘Šé …ç›®ï¼Œ
        ä¸¦æå–å…¬å‘Šçš„æ¨™é¡Œã€é€£çµï¼Œæœ€å¾Œç”¢ç”Ÿ AnnouncementArticleã€‚

        Args:
            response (scrapy.http.Response): ä¸‹è¼‰å™¨è¿”å›çš„å›æ‡‰ç‰©ä»¶ã€‚

        Returns:
            list[AnnouncementArticle]: åŒ…å«è§£æå‡ºçš„å…¬å‘Šæ–‡ç« è³‡è¨Šåˆ—è¡¨ã€‚
        """
        announcement_list_container = response.css("#pageptlist")
        announcements = announcement_list_container.css(
            ".row.listBS"
        )  # é¸å–æ¯å€‹å…¬å‘Šçš„ row

        articles = []

        for announcement in announcements:
            article = AnnouncementArticle()
            # æå–æ¨™é¡Œ
            link_selector = announcement.css(".mtitle a")  # ç›´æ¥é¸å– mtitle ä¸‹çš„ a æ¨™ç±¤
            article["title"] = (
                link_selector.css("::text").get().strip() if link_selector else None
            )
            # æå–é€£çµ
            relative_link = (
                link_selector.css("::attr(href)").get() if link_selector else None
            )
            article["link"] = response.urljoin(relative_link) if relative_link else None
            # æå–æ—¥æœŸ
            date_selector = announcement.css(".mdate::text").get()
            article["date"] = self.process_date(date_selector)

            # åªæœ‰ç•¶æ–‡ç« æœ‰æ¨™é¡Œå’Œå–®ä½æ™‚æ‰åŠ å…¥åˆ—è¡¨
            if article.get("title"):
                articles.append(dict(article))

        return articles

    def process_date(self, date_text: str | None) -> str | None:
        """
        è™•ç†æ—¥æœŸå­—ä¸²ã€‚

        æ­¤æ–¹æ³•æ¥æ”¶åŸå§‹æ—¥æœŸå­—ä¸²ï¼Œç§»é™¤å‰å¾Œç©ºç™½ä¸¦è¿”å›è™•ç†å¾Œçš„æ—¥æœŸå­—ä¸²ã€‚
        è‹¥è¼¸å…¥ç‚º Noneï¼Œå‰‡ç›´æ¥è¿”å› Noneã€‚

        Args:
            date_text (str | None): åŸå§‹æ—¥æœŸå­—ä¸²ã€‚

        Returns:
            str | None: è™•ç†å¾Œçš„æ—¥æœŸå­—ä¸²ï¼Œæˆ– Noneã€‚
        """
        if date_text is None:
            return None
        return date_text.strip()

    def process_link(self, link_selector, response) -> tuple[str | None, str | None]:
        """
        è™•ç†é€£çµå…ƒç´ ï¼Œæå–æ¨™é¡Œèˆ‡ hrefã€‚

        æ­¤æ–¹æ³•æ¥æ”¶é€£çµå…ƒç´ çš„ selectorï¼Œæå–é€£çµæ–‡å­—ä½œç‚ºæ¨™é¡Œï¼Œä¸¦æå– href å±¬æ€§ä½œç‚ºé€£çµã€‚
        åŒæ™‚è™•ç†ç›¸å°è·¯å¾‘ï¼Œç¢ºä¿é€£çµç‚ºçµ•å°è·¯å¾‘ã€‚

        Args:
            link_selector: é€£çµå…ƒç´ çš„ CSS selectorã€‚
            response (scrapy.http.Response): ä¸‹è¼‰å™¨è¿”å›çš„å›æ‡‰ç‰©ä»¶ï¼Œç”¨æ–¼è™•ç†ç›¸å°è·¯å¾‘ã€‚

        Returns:
            tuple[str | None, str | None]: åŒ…å«æ¨™é¡Œå’Œé€£çµçš„ tupleã€‚
        """
        title = link_selector.css("::text").get()
        href = link_selector.css("::attr(href)").get()

        if href:
            link = response.urljoin(href)
        else:
            link = None

        if title:
            title = title.strip() if title else None
            title = title.replace('"', "")  # ç§»é™¤æ¨™é¡Œä¸­çš„é›™å¼•è™Ÿï¼Œé¿å… JSON éŒ¯èª¤

        return title, link


class JsonPipeline:
    """
    Scrapy Pipelineï¼Œç”¨æ–¼å°‡çˆ¬å–çš„ Item å„²å­˜ç‚º JSON æª”æ¡ˆã€‚
    åŒæ™‚æœƒåˆä½µæ‰€æœ‰å…¬å‘Šè³‡æ–™åˆ°ä¸€å€‹ç¸½åˆæª”æ¡ˆã€‚
    """

    def open_spider(self, spider):
        """
        Spider é–‹å•Ÿæ™‚åŸ·è¡Œï¼Œå»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾èˆ‡åˆå§‹åŒ–åˆ—è¡¨ã€‚
        """
        COMBINED_JSON_FILE.parent.mkdir(parents=True, exist_ok=True)
        self.combined_data = []  # åˆå§‹åŒ–ç‚ºä¸€å€‹ç©ºçš„åˆ—è¡¨

    def process_item(self, item, spider):
        """
        è™•ç†æ¯ä¸€å€‹ Itemï¼Œå„²å­˜å…¬å‘Šè³‡æ–™åˆ° JSON æª”æ¡ˆã€‚

        Args:
            item (AnnouncementItem): çˆ¬å–åˆ°çš„å…¬å‘Šè³‡æ–™
            spider (AnnouncementsSpider): çˆ¬èŸ²å¯¦ä¾‹

        Returns:
            AnnouncementItem: è™•ç†å¾Œçš„ Item
        """
        serializable_item = dict(item)
        spider.logger.info(f"âœ… æˆåŠŸå„²å­˜ã€{item['department']}/{item['title']}ã€‘è³‡æ–™")
        spider.logger.debug(serializable_item)
        self.combined_data.append(serializable_item)

        return item

    def close_spider(self, spider):
        """
        Spider é—œé–‰æ™‚åŸ·è¡Œï¼Œåˆä½µæ‰€æœ‰å…¬å‘Šè³‡æ–™åˆ° JSON ç¸½åˆæª”æ¡ˆã€‚
        """
        # ä½¿ç”¨ link æ’åº
        sorted_data = sorted(self.combined_data, key=lambda x: x["link"])

        with open(COMBINED_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted_data, f, ensure_ascii=False, indent=4)  # ç›´æ¥ dump åˆ—è¡¨
        spider.logger.info(f'âœ… æˆåŠŸå„²å­˜æ‰€æœ‰å…¬å‘Šè³‡æ–™è‡³ "{COMBINED_JSON_FILE}"')
