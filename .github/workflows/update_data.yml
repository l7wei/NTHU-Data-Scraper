name: Crawl and update data

on:
  push:
    branches:
      - main
  schedule:
    - cron: "0 */2 * * *" # ÊØèÂÖ©Â∞èÊôÇÂü∑Ë°å‰∏ÄÊ¨° (ÂÉÖ ubuntu)
  workflow_dispatch:
    inputs:
      crawl_type:
        description: 'Type of crawl to perform'
        required: true
        default: 'incremental'
        type: choice
        options:
          - incremental
          - full

env:
  TZ: Asia/Taipei
  DATA_FOLDER: data

jobs:
  crawl_ubuntu:
    name: Crawl on ubuntu-latest
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.13.2
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run ubuntu spiders
        id: ubuntu_crawl
        shell: bash
        env:
          DATA_FOLDER: ${{ env.DATA_FOLDER }}
          CRAWL_TYPE: ${{ github.event.inputs.crawl_type || 'incremental' }}
        run: |
          # Inline crawl logic for ubuntu spiders
          UBUNTU_SPIDERS=(
            "nthu_announcements"
            "nthu_buses"
            "nthu_courses"
            "nthu_dining"
          )
          SUCCESS_COUNT=0
          FAIL_COUNT=0
          FAILED_SPIDERS=()
          
          echo "Running ${#UBUNTU_SPIDERS[@]} spider(s) with crawl type: ${CRAWL_TYPE}"
          
          for spider in "${UBUNTU_SPIDERS[@]}"; do
            echo "=== Crawling $spider ==="
            if python -m scrapy crawl "$spider" -a crawl_type="${CRAWL_TYPE}"; then
              ((SUCCESS_COUNT++))
            else
              ((FAIL_COUNT++))
              FAILED_SPIDERS+=("$spider")
            fi
          done
          
          if [ $FAIL_COUNT -gt 0 ]; then
            echo "Completed with $FAIL_COUNT failure(s): ${FAILED_SPIDERS[*]}" >&2
            exit 1
          fi
          
          echo "All spiders completed successfully ($SUCCESS_COUNT run)."
      - name: Upload ubuntu dataset
        if: ${{ success() }}
        uses: actions/upload-artifact@v4
        with:
          name: ubuntu-data
          path: ${{ env.DATA_FOLDER }}
          if-no-files-found: error

  crawl_self_hosted:
    name: Crawl on self-hosted (Manual Trigger Only)
    runs-on: self-hosted
    # Only run on manual workflow_dispatch
    if: github.event_name == 'workflow_dispatch'
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.13.2
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run self-hosted spiders
        id: self_hosted_crawl
        shell: bash
        env:
          DATA_FOLDER: ${{ env.DATA_FOLDER }}
          CRAWL_TYPE: ${{ github.event.inputs.crawl_type || 'full' }}
        run: |
          # Inline crawl logic for self-hosted spiders
          SELF_HOSTED_SPIDERS=(
            "nthu_directory"
            "nthu_maps"
            "nthu_newsletters"
          )
          SUCCESS_COUNT=0
          FAIL_COUNT=0
          FAILED_SPIDERS=()
          
          echo "Running ${#SELF_HOSTED_SPIDERS[@]} spider(s) with crawl type: ${CRAWL_TYPE}"
          
          for spider in "${SELF_HOSTED_SPIDERS[@]}"; do
            echo "=== Crawling $spider ==="
            if python -m scrapy crawl "$spider" -a crawl_type="${CRAWL_TYPE}"; then
              ((SUCCESS_COUNT++))
            else
              ((FAIL_COUNT++))
              FAILED_SPIDERS+=("$spider")
            fi
          done
          
          if [ $FAIL_COUNT -gt 0 ]; then
            echo "Completed with $FAIL_COUNT failure(s): ${FAILED_SPIDERS[*]}" >&2
            exit 1
          fi
          
          echo "All spiders completed successfully ($SUCCESS_COUNT run)."
      - name: Upload self-hosted dataset
        if: ${{ success() }}
        uses: actions/upload-artifact@v4
        with:
          name: self-hosted-data
          path: ${{ env.DATA_FOLDER }}
          if-no-files-found: error

  commit_changes:
    needs: [crawl_ubuntu, crawl_self_hosted]
    if: ${{ always() }}
    name: Commit JSON updates
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
      - name: Verify crawl availability
        id: crawl_guard
        env:
          UB_SUCCESS: ${{ needs.crawl_ubuntu.result == 'success' }}
          SH_SUCCESS: ${{ needs.crawl_self_hosted.result == 'success' }}
        run: |
          if [ "${UB_SUCCESS}" != "true" ] && [ "${SH_SUCCESS}" != "true" ]; then
            echo "No crawl job succeeded; aborting." >&2
            exit 1
          fi
      - name: Download ubuntu dataset
        if: ${{ needs.crawl_ubuntu.result == 'success' }}
        uses: actions/download-artifact@v4
        with:
          name: ubuntu-data
          path: ubuntu-data
      - name: Download self-hosted dataset
        if: ${{ needs.crawl_self_hosted.result == 'success' }}
        uses: actions/download-artifact@v4
        with:
          name: self-hosted-data
          path: self-hosted-data
      - name: Merge data artifacts
        shell: bash
        run: |
          mkdir -p "${DATA_FOLDER}"
          for source in ubuntu-data self-hosted-data; do
            if [ -d "$source/${DATA_FOLDER}" ]; then
              rsync -a "$source/${DATA_FOLDER}/" "${DATA_FOLDER}/"
            fi
          done
      - name: Configure Git
        run: |
          git config user.name "GitHub Actions"
          git config user.email "action@github.com"
      - name: Commit data updates
        shell: bash
        run: |
          git add "$DATA_FOLDER"
          changed=$(git diff --cached --name-only "$DATA_FOLDER")
          if [[ -n "$changed" ]]; then
            cats=$(echo "$changed" \
              | sed "s|^$DATA_FOLDER/||" \
              | cut -d '/' -f1 \
              | sed 's/\.json$//' \
              | sort -u \
              | paste -sd ",")
            git commit -m "üìù Scheduled update of JSON data [$cats] [skip ci]"
            git push
          else
            echo "No data changes."
          fi

  deploy_to_github:
    needs: commit_changes
    name: Deploy data to GitHub Pages
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout main branch (full history)
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.13.2

      - name: Generate file details
        run: python generate_file_detail.py --data_folder=$DATA_FOLDER --json_path=$DATA_FOLDER/file_details.json

      - name: Generate index.html
        run: python generate_index.py --json_path=$DATA_FOLDER/file_details.json --output=$DATA_FOLDER/index.html

      - name: Deploy to gh-pages branch
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ${{ env.DATA_FOLDER }}
          user_name: GitHub Actions
          user_email: action@github.com
          commit_message: "üöÄ Deployed to gh-pages"
